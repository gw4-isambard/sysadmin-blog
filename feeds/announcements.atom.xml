<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>GW4 Isambard - announcements</title><link href="/" rel="alternate"></link><link href="/feeds/announcements.atom.xml" rel="self"></link><id>/</id><updated>2019-08-22T00:00:00+01:00</updated><entry><title>New sysadmin website</title><link href="/new-sysadmin-website.html" rel="alternate"></link><published>2019-08-22T00:00:00+01:00</published><updated>2019-08-22T00:00:00+01:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2019-08-22:/new-sysadmin-website.html</id><content type="html">&lt;p&gt;You can look forward to seeing updates from the Isambard sysadmin on various changes and challenges surrounding the GW4 Isambard service!&lt;/p&gt;</content></entry><entry><title>Reoccuring Maintenance Window for Tuesday 0900-1000</title><link href="/reoccuring-maintenance-window-for-tuesday-0900-1000.html" rel="alternate"></link><published>2019-07-04T00:00:00+01:00</published><updated>2019-07-04T00:00:00+01:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2019-07-04:/reoccuring-maintenance-window-for-tuesday-0900-1000.html</id><summary type="html">&lt;p&gt;A maintenance window for the GW4 Isambard service has shall open every Tuesday from 09:00 to 11:00. This includes the XC50 (XCI, Phase 2), CS400 (Phase 1), storage, software changes &amp;amp; hardware work.&lt;/p&gt;
&lt;p&gt;Preferably disruption will be kept to a minimum and contained within this window. Unavailble nodes may …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A maintenance window for the GW4 Isambard service has shall open every Tuesday from 09:00 to 11:00. This includes the XC50 (XCI, Phase 2), CS400 (Phase 1), storage, software changes &amp;amp; hardware work.&lt;/p&gt;
&lt;p&gt;Preferably disruption will be kept to a minimum and contained within this window. Unavailble nodes may be brought back online outside of this window to avoid prolonged degraded capacity.&lt;/p&gt;</content></entry><entry><title>Skylake nodes installed</title><link href="/skylake-nodes-installed.html" rel="alternate"></link><published>2019-07-03T00:00:00+01:00</published><updated>2019-07-03T00:00:00+01:00</updated><author><name>Isambard sysadmins</name></author><id>tag:None,2019-07-03:/skylake-nodes-installed.html</id><summary type="html">&lt;p&gt;Following the removal of our Arm early access nodes, we have finally installed two nodes of Intel Skylake 2x Intel Xeon Gold 6152 "Skylake" 22-core @ 2.10GHz&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://gw4-isambard.github.io/docs/user-guide/phase1.html#intel-skylake"&gt;https://gw4-isambard.github.io/docs/user-guide/phase1.html#intel-skylake&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lustre is available and basic compilers such as GCC and Clang from the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Following the removal of our Arm early access nodes, we have finally installed two nodes of Intel Skylake 2x Intel Xeon Gold 6152 "Skylake" 22-core @ 2.10GHz&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://gw4-isambard.github.io/docs/user-guide/phase1.html#intel-skylake"&gt;https://gw4-isambard.github.io/docs/user-guide/phase1.html#intel-skylake&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lustre is available and basic compilers such as GCC and Clang from the CentOS repositories have been installed.&lt;/p&gt;</content></entry><entry><title>Isambard XC50 software &amp; chip updates</title><link href="/isambard-xc50-software-chip-updates.html" rel="alternate"></link><published>2019-03-05T00:00:00+00:00</published><updated>2019-03-05T00:00:00+00:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2019-03-05:/isambard-xc50-software-chip-updates.html</id><summary type="html">&lt;p&gt;Next week (w/c 11th March), we will be shutting down Isambard Phase 2 to upgrade both the hardware and system software. This process will likely take the full week, and so we expect to resume service from Monday 18th March.&lt;/p&gt;
&lt;p&gt;These upgrades will move us to Cray Linux Environment …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Next week (w/c 11th March), we will be shutting down Isambard Phase 2 to upgrade both the hardware and system software. This process will likely take the full week, and so we expect to resume service from Monday 18th March.&lt;/p&gt;
&lt;p&gt;These upgrades will move us to Cray Linux Environment 7.0, which bumps the underlying Operating System from SLES 12 to SLES 15. It is expected that this may cause existing dynamically linked executables to fail to run, so we recommend recompiling any such programs that you may have once the upgrade is completed. Note that after the upgrade, the minimum available Cray Developer Toolkit will be CDT 19.03, and so you may encounter some issues when recompiling your codes.&lt;/p&gt;
&lt;h2&gt;Updates&lt;/h2&gt;
&lt;p&gt;14 March: The upgrade is proceeding smoothly, XCI is running it's new software stack on fresh chips! We are working now to ensure the service is stable, configured correctly for user access and any hardware bugs are caught early.&lt;/p&gt;
&lt;div class="gallery"&gt;
    &lt;a title="XC50 TX2 blade close-up" href="/images/xc50-tx2-blade-close-up-1.jpg"&gt;
        &lt;img src="/images/xc50-tx2-blade-close-up-1.jpg" alt="xc50-tx2-blade-close-up"&gt;
    &lt;/a&gt;
    &lt;a title="XC50 TX2 blade close-up 2" href="/images/xc50-tx2-blade-close-up-2.jpg"&gt;
        &lt;img src="/images/xc50-tx2-blade-close-up-2.jpg" alt="xc50-tx2-blade-close-up"&gt;
    &lt;/a&gt;
&lt;/div&gt;</content></entry><entry><title>New documentation!</title><link href="/new-documentation.html" rel="alternate"></link><published>2018-10-16T00:00:00+01:00</published><updated>2018-10-16T00:00:00+01:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2018-10-16:/new-documentation.html</id><content type="html">&lt;p&gt;New Isambard user documentation source! =&amp;gt; &lt;a href="https://gw4-isambard.github.io/docs/"&gt;https://gw4-isambard.github.io/docs/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you want to contribute we're happy to review pull requests, or just email changes your local GW4 sysadmin!&lt;/p&gt;</content></entry><entry><title>IBM Power 9 with Nvidia "Volta" V100</title><link href="/ibm-power-9-with-nvidia-volta-v100.html" rel="alternate"></link><published>2018-06-08T00:00:00+01:00</published><updated>2018-06-08T00:00:00+01:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2018-06-08:/ibm-power-9-with-nvidia-volta-v100.html</id><summary type="html">&lt;p&gt;Freshly installed, Isambard has another couple of architectures available!&lt;/p&gt;
&lt;p&gt;SSH onto a login node as normal, documented at https://github.com/UoB-HPC/GW4-Isambard/blob/master/docs/GettingStarted.md - Then SSH to &lt;code&gt;power-001&lt;/code&gt; or &lt;code&gt;power-002&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;2x IBM Power System AC922:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2x 20-core 2.4 GHz (3.0 GHz Turbo) Power 9 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Freshly installed, Isambard has another couple of architectures available!&lt;/p&gt;
&lt;p&gt;SSH onto a login node as normal, documented at https://github.com/UoB-HPC/GW4-Isambard/blob/master/docs/GettingStarted.md - Then SSH to &lt;code&gt;power-001&lt;/code&gt; or &lt;code&gt;power-002&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;2x IBM Power System AC922:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2x 20-core 2.4 GHz (3.0 GHz Turbo) Power 9 CPU Little-Endian&lt;/li&gt;
&lt;li&gt;256GiB DDR4&lt;/li&gt;
&lt;li&gt;EDR Infiniband&lt;/li&gt;
&lt;li&gt;2x NVIDIA "Volta" V100 GPU with NVLink&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Current software:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Red Hat 7.5 (for IBM Power Little Endian)&lt;/li&gt;
&lt;li&gt;IBM XL C/C++ 16.1.0&lt;/li&gt;
&lt;li&gt;IBM XL Fortran 16.1.0&lt;/li&gt;
&lt;li&gt;GCC 4.8.5&lt;/li&gt;
&lt;li&gt;NVIDIA drivers 396.26 (Upstream)&lt;/li&gt;
&lt;li&gt;NVIDIA CUDA 9.2&lt;/li&gt;
&lt;li&gt;NVIDIA CuDNN 7.1.4&lt;/li&gt;
&lt;li&gt;NVIDIA NCCL 2.2.13&lt;/li&gt;
&lt;li&gt;Anaconda2 5.1.0&lt;/li&gt;
&lt;li&gt;Anaconda3 5.1.0&lt;/li&gt;
&lt;li&gt;POWER AI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lustre is mounted via NFS at /lustre-nfs with /home, /projects/, /common mounted through this. In the near future I hope to have a direct Lustre mount without NFS, this is pending an upgrade to Isambard's Lustre filesystem.&lt;/p&gt;
&lt;p&gt;PBS is not enabled on these nodes currently, we are looking into this! For now the nodes will be a free-for-all so please experiment considerately as-always!&lt;/p&gt;
&lt;p&gt;Please provide feedback if you encounter any issues or have any requests!&lt;/p&gt;</content></entry><entry><title>XC50 Approaches...</title><link href="/xc50-approaches.html" rel="alternate"></link><published>2018-06-07T00:00:00+01:00</published><updated>2018-06-07T00:00:00+01:00</updated><author><name>Joe Heaton</name></author><id>tag:None,2018-06-07:/xc50-approaches.html</id><summary type="html">&lt;p&gt;The much anticipated Cray XC50 cabinet is within a couple of months of shipping and I just wanted to publish some specs!&lt;/p&gt;
&lt;p&gt;The single cabinet consists of approx 164 compute nodes of 64 cores each, for a total of 10'496 cores of Cavium Thunder X2 ARMv8, backed by the same …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The much anticipated Cray XC50 cabinet is within a couple of months of shipping and I just wanted to publish some specs!&lt;/p&gt;
&lt;p&gt;The single cabinet consists of approx 164 compute nodes of 64 cores each, for a total of 10'496 cores of Cavium Thunder X2 ARMv8, backed by the same Aries interconnect found in all our Cray systems. A 0.5Petabyte Lustre filesystem is dedicated to the Isambard system.&lt;/p&gt;
&lt;p&gt;Discussions are underway on acceptance tests, we expect to run HPL (LINPACK), HPCG, STREAM, MPI &amp;amp; I/O benchmarks. Some practical codes will also be run for comparison against the numbers produced on the Early Access nodes ( &lt;a href="http://www.goingarm.com/slides/2017/SC17/GoingArm_SC17_Bristol_Isambard.pdf"&gt;http://www.goingarm.com/slides/2017/SC17/GoingArm_SC17_Bristol_Isambard.pdf&lt;/a&gt; ), including UM/NEMO, a chemistry and an engineering code.&lt;/p&gt;
&lt;p&gt;The HPC group at Bristol Uni has recently put out a paper on these numbers in more depth: &lt;a href="https://uob-hpc.github.io/assets/cug-2018.pdf"&gt;https://uob-hpc.github.io/assets/cug-2018.pdf&lt;/a&gt;&lt;/p&gt;</content></entry></feed>